{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [目标] 让机器学会理解语言的情感\n",
    "\n",
    "* 其实每一个字对于机器来说都是没有意义的，要怎么让机器做到情感理解呢？\n",
    "* 想办法把文字转成机器可以理解的方式\n",
    "* 会带到以下观念：\n",
    "    - 断词系统\n",
    "    - 神奇的词项量\n",
    "    - 建立一个语意分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## 深度学习框架 PyTorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 资料集介绍\n",
    "\n",
    "> 语意的训练资料，也是需要标记的，许多网站服务上，会有使用者的评论以及给分或是打星，这些都是很棒的训练资料！因为可以很清楚了解到使用者在留言这句话的时候，是觉得很开心很满意，或是非常的不满。可以透过大数据课程中学到的爬虫技巧，抓取各式各样的模型训练资料！\n",
    "\n",
    "### 因为时间有限，网路上也有很多热心人士准备好的研究资料，例如下面这些：\n",
    "* 携程网顾客评论资料：\n",
    "    * https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb\n",
    "* 复旦大学文本分类语料库测试语料\n",
    "    * http://www.nlpir.org/wordpress/2017/10/02/%e6%96%87%e6%9c%ac%e5%88%86%e7%b1%bb%e8%af%ad% e6%96%99%e5%ba%93%ef%bc%88%e5%a4%8d%e6%97%a6%ef%bc%89%e6%b5%8b%e8%af%95%e8% af%ad%e6%96%99/\n",
    "* 电商平台商品描述语料\n",
    "    * https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/online_shopping_10_cats/intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 就以携程网的饭店论资料来试试吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv('./data/hotel_commients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>总的来说，这样的酒店配这样的价格还算可以，希望他赶快装修，给我的客人留些好的印象</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>价格比比较不错的酒店。这次免费升级了，感谢前台服务员。房子还好，地毯是新的，比上次的好些。早...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>不错，在同等档次酒店中应该是值得推荐的！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>入住丽晶，感觉很好。因为是新酒店，的确有淡淡的油漆味，房间内较新。房间大小合适，卫生间设备齐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1。酒店比较新，装潢和设施还不错，只是房间有些油漆味。2。早餐还可以，只是品种不是很多。3。...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风\n",
       "5      1           总的来说，这样的酒店配这样的价格还算可以，希望他赶快装修，给我的客人留些好的印象\n",
       "6      1  价格比比较不错的酒店。这次免费升级了，感谢前台服务员。房子还好，地毯是新的，比上次的好些。早...\n",
       "7      1                               不错，在同等档次酒店中应该是值得推荐的！\n",
       "8      1  入住丽晶，感觉很好。因为是新酒店，的确有淡淡的油漆味，房间内较新。房间大小合适，卫生间设备齐...\n",
       "9      1  1。酒店比较新，装潢和设施还不错，只是房间有些油漆味。2。早餐还可以，只是品种不是很多。3。..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 看一下比较属正面的话\n",
    "\n",
    "res_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7756</th>\n",
       "      <td>0</td>\n",
       "      <td>到半夜竟然没暖气,怎么住啊????!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7757</th>\n",
       "      <td>0</td>\n",
       "      <td>房间比较差，尤其是洗手间，房间隔音和餐饮服务都不好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7758</th>\n",
       "      <td>0</td>\n",
       "      <td>不好的，319房间有故臭味。要求换房说满了，我是3月去的。在路上认识了一个上海人，他说他退房...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>0</td>\n",
       "      <td>房间太小,宾馆有租四轮自行车的,很破,骑不动,后来服务员说他们的车都是东方绿舟淘汰的.酒巴装...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>0</td>\n",
       "      <td>我去年的时候去过该酒店，那时候住的是无烟房，感觉还好，这回去住的是高级标准间，进去了房间就有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>0</td>\n",
       "      <td>尼斯酒店的几大特点：噪音大、环境差、配置低、服务效率低。如：1、隔壁歌厅的声音闹至午夜3点许...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>0</td>\n",
       "      <td>盐城来了很多次，第一次住盐阜宾馆，我的确很失望整个墙壁黑咕隆咚的，好像被烟熏过一样家具非常的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>0</td>\n",
       "      <td>看照片觉得还挺不错的，又是4星级的，但入住以后除了后悔没有别的，房间挺大但空空的，早餐是有但...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>0</td>\n",
       "      <td>我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>0</td>\n",
       "      <td>说实在的我很失望，之前看了其他人的点评后觉得还可以才去的，结果让我们大跌眼镜。我想这家酒店以...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             review\n",
       "7756      0                        到半夜竟然没暖气,怎么住啊????!!!!!!!!!!\n",
       "7757      0                         房间比较差，尤其是洗手间，房间隔音和餐饮服务都不好。\n",
       "7758      0  不好的，319房间有故臭味。要求换房说满了，我是3月去的。在路上认识了一个上海人，他说他退房...\n",
       "7759      0  房间太小,宾馆有租四轮自行车的,很破,骑不动,后来服务员说他们的车都是东方绿舟淘汰的.酒巴装...\n",
       "7760      0  我去年的时候去过该酒店，那时候住的是无烟房，感觉还好，这回去住的是高级标准间，进去了房间就有...\n",
       "7761      0  尼斯酒店的几大特点：噪音大、环境差、配置低、服务效率低。如：1、隔壁歌厅的声音闹至午夜3点许...\n",
       "7762      0  盐城来了很多次，第一次住盐阜宾馆，我的确很失望整个墙壁黑咕隆咚的，好像被烟熏过一样家具非常的...\n",
       "7763      0  看照片觉得还挺不错的，又是4星级的，但入住以后除了后悔没有别的，房间挺大但空空的，早餐是有但...\n",
       "7764      0  我们去盐城的时候那里的最低气温只有4度，晚上冷得要死，居然还不开空调，投诉到酒店客房部，得到...\n",
       "7765      0  说实在的我很失望，之前看了其他人的点评后觉得还可以才去的，结果让我们大跌眼镜。我想这家酒店以..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 相对负面一些的话\n",
    "\n",
    "res_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 介绍一个好用的断词工具 jieba\n",
    "\n",
    "* https://github.com/fxsjy/jieba\n",
    "* 断词的学问博大精深，需要透过大量的语料建模而成\n",
    "    * 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n",
    "    * 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合\n",
    "    * 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n",
    "* 光理解背后的逻辑，得耗费大量的时间，站在巨人的肩膀上也要心怀感激！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.39)\n",
      "\u001b[31mfastai 1.0.59 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试刀一下，怎么好像怪怪的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.810 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 。 酒店 比较 新 ， 装潢 和 设施 还 不错 ， 只是 房间 有些 油漆味 。 2 。 早餐 还 可以 ， 只是 品种 不是 很多 。 3 。 交通 比较 方便 ， 周围 的 小 饭店 比较 多 。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(jieba.lcut(res_df.iloc[9].review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在自然语言处理的领域里，资料清洗是非常重要的一环！垃圾进垃圾出，是无法学习出好的模型的\n",
    "\n",
    "from src import textutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence(sentence):   \n",
    "    return [w for w in jieba.lcut(textutil.clean_sentence(sentence)) if w != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 酒店 比较 新 装潢 和 设施 还 不错 只是 房间 有些 油漆味 2 早餐 还 可以 只是 品种 不是 很多 3 交通 比较 方便 周围 的 小 饭店 比较 多'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(cut_sentence(res_df.iloc[9].review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支援不同的断词模式\n",
    "\n",
    "还可以自行加载自定义词库\n",
    "\n",
    "```python\n",
    "jieba.load_userdict(file_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "Search Mode: 小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造\n"
     ]
    }
   ],
   "source": [
    "# 全模式\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  \n",
    "\n",
    "# 精确模式\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  \n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  \n",
    "print(\"Search Mode: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews, all_labels = [], []\n",
    "\n",
    "res_df = res_df.astype({'review': 'str'})\n",
    "corpus = []\n",
    "for idx, row in res_df.iterrows():\n",
    "    result = cut_sentence(row.review)\n",
    "    corpus.append(result)\n",
    "    all_reviews.append(result) \n",
    "    all_labels.append(row.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['距离',\n",
       "  '川沙',\n",
       "  '公路',\n",
       "  '较近',\n",
       "  '但是',\n",
       "  '公交',\n",
       "  '指示',\n",
       "  '不',\n",
       "  '对',\n",
       "  '如果',\n",
       "  '是',\n",
       "  '蔡陆线',\n",
       "  '的话',\n",
       "  '会',\n",
       "  '非常',\n",
       "  '麻烦',\n",
       "  '建议',\n",
       "  '用',\n",
       "  '别的',\n",
       "  '路线',\n",
       "  '房间',\n",
       "  '较为简单'],\n",
       " ['商务', '大床', '房', '房间', '很大', '床有', '2m', '宽', '整体', '感觉', '经济', '实惠', '不错'],\n",
       " ['早餐',\n",
       "  '太',\n",
       "  '差',\n",
       "  '无论',\n",
       "  '去',\n",
       "  '多少',\n",
       "  '人',\n",
       "  '那边',\n",
       "  '也',\n",
       "  '不加',\n",
       "  '食品',\n",
       "  '的',\n",
       "  '酒店',\n",
       "  '应该',\n",
       "  '重视',\n",
       "  '一下',\n",
       "  '这个',\n",
       "  '问题',\n",
       "  '了',\n",
       "  '房间',\n",
       "  '本身',\n",
       "  '很',\n",
       "  '好']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 断词只是第一步，机器怎么知道这些词的意义呢？\n",
    "\n",
    "试问一下，下列的词汇，意思是相近还是相远的呢？\n",
    "* 好吃跟难吃\n",
    "* 美女与野兽\n",
    "* 天使与恶魔\n",
    "* 李白跟杜甫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强大的工具「词向量」(word2vec) 来了\n",
    "\n",
    "概念：\n",
    "* 有些字很常出现在一块，有些却很少，如果大量统计出这种共同出现的机率，就有机会推测出那些词的意义是相近的，而哪些是相远的！\n",
    "\n",
    "#### 继续站在巨人的肩膀上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.10.19)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.20.0)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "\u001b[31mfastai 1.0.59 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数解释\n",
    "\n",
    "* size: 词向量的维度\n",
    "* window: 词的关联区间定义\n",
    "* min_count: 出现几次的词才加入训练 (当语料非常大，或是资料很脏的时候)\n",
    "* workers: 用多少个CPU核心训练\n",
    "* alpha: 学习率(learning rate)\n",
    "* min_alpha: 学习率最小限制\n",
    "* iter: 训练回合数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21250045, 26058100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = w2v.Word2Vec(size=100,  \n",
    "                     window=5, \n",
    "                     min_count=1, \n",
    "                     workers=multiprocessing.cpu_count(),\n",
    "                     alpha=0.025, min_alpha=0.001, iter=50)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, epochs=model.iter, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明亮\n",
      "布置\n",
      "舒适\n",
      "宽敞明亮\n",
      "整洁\n",
      "超大\n",
      "气派\n",
      "干净\n",
      "很大\n",
      "安静\n"
     ]
    }
   ],
   "source": [
    "word = '宽敞'\n",
    "slist = model.wv.most_similar(positive=[word],topn=10)\n",
    "for word , score in slist :\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店\n",
      "宾馆\n",
      "度假村\n",
      "城市\n",
      "徽派\n",
      "旅馆\n",
      "餐馆\n",
      "餐饮\n",
      "山庄\n",
      "店\n"
     ]
    }
   ],
   "source": [
    "word = ['饭店']\n",
    "slist = model.wv.most_similar(positive=word,topn=10)\n",
    "for word , score in slist :\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./models', exist_ok=True)\n",
    "model.save(\"./models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [试试看] 调整看看 Word2Vec 里面的各式参数，并分享心得\n",
    "\n",
    "* 重要的参数有哪些呢？\n",
    "* 修改过参数有什么影响吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备朝语意理解迈进拉！\n",
    "\n",
    "* 先把训练资料准备好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "word_counts = model.wv.vectors.shape[0]\n",
    "\n",
    "for word in model.wv.vocab:\n",
    "    word_list.append(word)\n",
    "    \n",
    "vocab_to_int = {word:model.wv.vocab[word].index for word in word_list}\n",
    "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
    "int_to_vocab[word_counts] = '###'\n",
    "vocab_to_int['###'] = word_counts\n",
    "encoded_reviews = [[vocab_to_int[word] for word in review] for review in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['距离',\n",
       " '川沙',\n",
       " '公路',\n",
       " '较近',\n",
       " '但是',\n",
       " '公交',\n",
       " '指示',\n",
       " '不',\n",
       " '对',\n",
       " '如果',\n",
       " '是',\n",
       " '蔡陆线',\n",
       " '的话',\n",
       " '会',\n",
       " '非常',\n",
       " '麻烦',\n",
       " '建议',\n",
       " '用',\n",
       " '别的',\n",
       " '路线',\n",
       " '房间',\n",
       " '较为简单']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[423,\n",
       " 14211,\n",
       " 3934,\n",
       " 2542,\n",
       " 45,\n",
       " 1551,\n",
       " 2770,\n",
       " 13,\n",
       " 54,\n",
       " 90,\n",
       " 3,\n",
       " 14212,\n",
       " 215,\n",
       " 65,\n",
       " 30,\n",
       " 864,\n",
       " 150,\n",
       " 105,\n",
       " 639,\n",
       " 2451,\n",
       " 5,\n",
       " 14213]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 看得懂这是什么吗？\n",
    "\n",
    "encoded_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 资料清理\n",
    "\n",
    "* 排除掉可能长度是零的句子\n",
    "* 透过 `shuffle` 增加排序的乱度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7766\n",
      "7766\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = np.array( [label for idx, label in enumerate(all_labels) if len(encoded_reviews[idx]) > 0] )\n",
    "encoded_reviews = np.array( [review for review in encoded_reviews if len(review) > 0])\n",
    "\n",
    "s = np.arange(encoded_labels.shape[0])\n",
    "np.random.shuffle(s)\n",
    "\n",
    "encoded_labels = encoded_labels[s]\n",
    "encoded_reviews = encoded_reviews[s]\n",
    "\n",
    "\n",
    "print(len(encoded_labels))\n",
    "print(len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子长度补齐\n",
    "\n",
    "* 为什么呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(encoded_reviews, seq_length):\n",
    "    reviews = []\n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([word_counts]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "\n",
    "padded_reviews = pad_text(encoded_reviews, seq_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329, 29329,\n",
       "       29329,   103,    10,    25,  5172,   175,    37,     0,    29,\n",
       "         558,    79,   145,   236,     0,   299,   108,   196,   256,\n",
       "       20600,     1,  1475,    98,   289,    15,     4,    14,   123,\n",
       "          10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分 `訓練集` 以及 `測試集`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6212, 100) (6212,)\n",
      "(1554, 100) (1554,)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "total = padded_reviews.shape[0]\n",
    "train_cutoff = int(total * train_ratio)\n",
    "\n",
    "train_x, train_y = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\n",
    "test_x, test_y = padded_reviews[train_cutoff:], encoded_labels[train_cutoff:]\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始建模了\n",
    "\n",
    "* pytorch 的模型有固定的写法\n",
    "    * 先定义网路层的内容\n",
    "    * 再定义网路的架构\n",
    "\n",
    "```python\n",
    "class MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL, self).__init__()\n",
    "        self.lstm = nn.LSTM()\n",
    "        self.fc = nn.Linear()\n",
    "        \n",
    "    def forward(self):\n",
    "        out = self.lstm()\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_SIMPLE(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, drop_p = 0.5):\n",
    "        super(LSTM_SIMPLE, self).__init__()\n",
    "        self.n_vocab = n_vocab     \n",
    "        self.n_hidden = n_hidden   \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, 1, batch_first = True, \n",
    "                            bidirectional=False, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    " \n",
    "    def forward(self, input_words):\n",
    "        embeds = self.embedding(input_words)    \n",
    "        lstm_out, _ = self.lstm(embeds, None)         \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)[:, -1, :]                     \n",
    "        \n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把 `numpy` 资料转换为 PyTorch 的 Tensor 型态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_t = torch.from_numpy(train_x).type(torch.LongTensor)\n",
    "train_y_t = torch.from_numpy(train_y).type(torch.LongTensor)\n",
    "\n",
    "test_x_t = torch.from_numpy(test_x).type(torch.LongTensor)\n",
    "test_y_t = torch.from_numpy(test_y).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 透过 PyTorch DataLoader 模组，做到批次训练的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_dataset = Data.TensorDataset(train_x_t, train_y_t)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,  # pytorch TensorDataset 格式\n",
    "    batch_size=BATCH_SIZE,  # 透过 batch size 设定批次处理\n",
    "    shuffle=True,           # shuffle 可在每个 epoc 都打乱资料顺序\n",
    "    num_workers=4,          # 使用几个 CPU 线程做运算\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 100\n",
    "n_hidden = 64\n",
    "n_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看一下网路架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_SIMPLE(\n",
      "  (embedding): Embedding(29330, 100)\n",
      "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_s = LSTM_SIMPLE(n_vocab, n_embed, n_hidden, n_output)\n",
    "print(net_s)  \n",
    "\n",
    "optimizer = torch.optim.Adam(net_s.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "plt.ion()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.7190 | test accuracy: 0.64\n",
      "Epoch:  0 | train loss: 0.6759 | test accuracy: 0.68\n",
      "Epoch:  0 | train loss: 0.6363 | test accuracy: 0.71\n",
      "Epoch:  0 | train loss: 0.7152 | test accuracy: 0.71\n",
      "Epoch:  1 | train loss: 0.4450 | test accuracy: 0.80\n",
      "Epoch:  1 | train loss: 0.5015 | test accuracy: 0.79\n",
      "Epoch:  1 | train loss: 0.3811 | test accuracy: 0.77\n",
      "Epoch:  1 | train loss: 0.5694 | test accuracy: 0.80\n",
      "Epoch:  2 | train loss: 0.2369 | test accuracy: 0.78\n",
      "Epoch:  2 | train loss: 0.1497 | test accuracy: 0.82\n",
      "Epoch:  2 | train loss: 0.4146 | test accuracy: 0.80\n",
      "Epoch:  2 | train loss: 0.3402 | test accuracy: 0.82\n",
      "Epoch:  3 | train loss: 0.2659 | test accuracy: 0.82\n",
      "Epoch:  3 | train loss: 0.1055 | test accuracy: 0.82\n",
      "Epoch:  3 | train loss: 0.5010 | test accuracy: 0.82\n",
      "Epoch:  3 | train loss: 0.2424 | test accuracy: 0.82\n",
      "Epoch:  4 | train loss: 0.1083 | test accuracy: 0.82\n",
      "Epoch:  4 | train loss: 0.1214 | test accuracy: 0.82\n",
      "Epoch:  4 | train loss: 0.1001 | test accuracy: 0.82\n",
      "Epoch:  4 | train loss: 0.3739 | test accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):   \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  \n",
    "        out = net_s(batch_x)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()   # 清空 gradients\n",
    "        loss.backward()         # 倒传导参数训练\n",
    "        optimizer.step()        # 调整参数\n",
    "        \n",
    "        if step % 50 == 0:      # 每 50 步印一个结果\n",
    "            test_output = net_s(test_x_t)                   \n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 还记得 transfer learning 的概念吗？\n",
    "\n",
    "* 把词向量当成预设的 embeddings layer\n",
    "* 学习速度会加快，而且效果更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 记得配给  `###`  一个预设零向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(np.vstack([model.wv.vectors, np.zeros(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False  ## 把 embedding 层的参数固定住了\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_WV(nn.Module):\n",
    "    def __init__(self, weights, n_hidden, n_output, drop_p = 0.5):\n",
    "        super(LSTM_WV, self).__init__()\n",
    "        self.n_vocab = n_vocab     \n",
    "        self.n_hidden = n_hidden   \n",
    "        self.embedding, num_embeddings, n_embed = create_emb_layer(weights)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, 1, batch_first = True, \n",
    "                            bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden*2, n_output)\n",
    " \n",
    "    def forward(self, input_words):\n",
    "        embeds = self.embedding(input_words)    \n",
    "        lstm_out, _ = self.lstm(embeds, None)         \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)[:, -1, :]                     \n",
    "        \n",
    "        return fc_out\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(2, bsz, self.n_hidden),\n",
    "                weight.new_zeros(2, bsz, self.n_hidden))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_WV(\n",
      "  (embedding): Embedding(29330, 100)\n",
      "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_wv = LSTM_WV(weights, n_hidden, n_output)\n",
    "print(net_wv)  \n",
    "\n",
    "optimizer = torch.optim.Adam(net_wv.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "plt.ion()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.6818 | test accuracy: 0.67\n",
      "Epoch:  0 | train loss: 0.6104 | test accuracy: 0.71\n",
      "Epoch:  0 | train loss: 0.5192 | test accuracy: 0.76\n",
      "Epoch:  0 | train loss: 0.6178 | test accuracy: 0.77\n",
      "Epoch:  1 | train loss: 0.4363 | test accuracy: 0.79\n",
      "Epoch:  1 | train loss: 0.3275 | test accuracy: 0.79\n",
      "Epoch:  1 | train loss: 0.4032 | test accuracy: 0.79\n",
      "Epoch:  1 | train loss: 0.2616 | test accuracy: 0.80\n",
      "Epoch:  2 | train loss: 0.2454 | test accuracy: 0.79\n",
      "Epoch:  2 | train loss: 0.5057 | test accuracy: 0.81\n",
      "Epoch:  2 | train loss: 0.2937 | test accuracy: 0.80\n",
      "Epoch:  2 | train loss: 0.2072 | test accuracy: 0.81\n",
      "Epoch:  3 | train loss: 0.0336 | test accuracy: 0.81\n",
      "Epoch:  3 | train loss: 0.0364 | test accuracy: 0.81\n",
      "Epoch:  3 | train loss: 0.0557 | test accuracy: 0.81\n",
      "Epoch:  3 | train loss: 0.2125 | test accuracy: 0.82\n",
      "Epoch:  4 | train loss: 0.1494 | test accuracy: 0.80\n",
      "Epoch:  4 | train loss: 0.0683 | test accuracy: 0.81\n",
      "Epoch:  4 | train loss: 0.2154 | test accuracy: 0.80\n",
      "Epoch:  4 | train loss: 0.4183 | test accuracy: 0.79\n",
      "Epoch:  5 | train loss: 0.3401 | test accuracy: 0.79\n",
      "Epoch:  5 | train loss: 0.0414 | test accuracy: 0.81\n",
      "Epoch:  5 | train loss: 0.1373 | test accuracy: 0.80\n",
      "Epoch:  5 | train loss: 0.3472 | test accuracy: 0.80\n",
      "Epoch:  6 | train loss: 0.1614 | test accuracy: 0.80\n",
      "Epoch:  6 | train loss: 0.1761 | test accuracy: 0.81\n",
      "Epoch:  6 | train loss: 0.0658 | test accuracy: 0.81\n",
      "Epoch:  6 | train loss: 0.3855 | test accuracy: 0.80\n",
      "Epoch:  7 | train loss: 0.1188 | test accuracy: 0.80\n",
      "Epoch:  7 | train loss: 0.0441 | test accuracy: 0.80\n",
      "Epoch:  7 | train loss: 0.0438 | test accuracy: 0.80\n",
      "Epoch:  7 | train loss: 0.2240 | test accuracy: 0.81\n",
      "Epoch:  8 | train loss: 0.0620 | test accuracy: 0.80\n",
      "Epoch:  8 | train loss: 0.1697 | test accuracy: 0.81\n",
      "Epoch:  8 | train loss: 0.1086 | test accuracy: 0.80\n",
      "Epoch:  8 | train loss: 0.1292 | test accuracy: 0.81\n",
      "Epoch:  9 | train loss: 0.0693 | test accuracy: 0.78\n",
      "Epoch:  9 | train loss: 0.1188 | test accuracy: 0.80\n",
      "Epoch:  9 | train loss: 0.1096 | test accuracy: 0.80\n",
      "Epoch:  9 | train loss: 0.0344 | test accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "cur_loss = 10000\n",
    "cur_acc = 0\n",
    "\n",
    "for epoch in range(10):   \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  \n",
    "        net_wv.init_hidden(len(batch_y))\n",
    "        out = net_wv(batch_x)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()         \n",
    "        optimizer.step()        \n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            net_wv.init_hidden(len(batch_y))\n",
    "            test_output = net_wv(test_x_t)                   \n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
    "            if accuracy >= cur_acc and loss.item() <= cur_loss:   ### 只要模型效果优化了，就重新储存\n",
    "                torch.save(net_wv.state_dict(), f'./models/lstm_{epoch}_{step}.param')\n",
    "                cur_acc = accuracy\n",
    "                cur_loss = loss.item()\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LSTM_WV(weights, n_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.load_state_dict(torch.load('./models/lstm_4_150.param'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review, seq_length = 100):\n",
    "    words = cut_sentence(review)\n",
    "    encoded_words = [vocab_to_int[word] for word in words]\n",
    "    padded_words = pad_text([encoded_words], seq_length)\n",
    "    padded_words = torch.from_numpy(padded_words)\n",
    "    output = predictor(padded_words)\n",
    "    pred = torch.max(output, 1)[1].data.numpy()\n",
    "\n",
    "    msg = \"顾客很开心喔！\" if pred == 1 else \"服务要再加强了\"\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "顾客很开心喔！\n",
      "顾客很开心喔！\n",
      "顾客很开心喔！\n",
      "服务要再加强了\n",
      "服务要再加强了\n",
      "服务要再加强了\n",
      "顾客很开心喔！\n"
     ]
    }
   ],
   "source": [
    "review1 = \"闹中取静的一个地方，在窗前能看到不错的风景。\"\n",
    "review2 = \"位置不错,在市中心.周围吃饭等很方便.房间一如既往的干净\"\n",
    "review3 = \"这个宾馆好像是属于海军的南海舰队，地理位置也很好，靠近省委火车站，离黄兴步行街也就三站地，值得入住\"\n",
    "review4 = \"房间比较差，尤其是洗手间，房间隔音和餐饮服务都不好\"\n",
    "review5 = \"硬件差：房间设施简单、陈旧，房内有较重的异味，半夜后中央空调“罢工”。服务不好，房间差\"\n",
    "review6 = \"真是有够烂到爆，再也不想住了，太夸张，很差非常差\"\n",
    "review7 = \"这家餐厅真的很棒 很舒服 服务又好 下次一定会再来\"\n",
    "\n",
    "print(predict(review1)) \n",
    "print(predict(review2))\n",
    "print(predict(review3))\n",
    "print(predict(review4))\n",
    "print(predict(review5))\n",
    "print(predict(review6))\n",
    "print(predict(review7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
